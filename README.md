# Flash Attention

This repo re‑implements FlashAttention‑2 kernels from scratch and benchmarks them against PyTorch’s builtin fused attention.