# Flash Attention

This repo re‑implements FlashAttention‑2 kernels from scratch and benchmarks them against PyTorch’s builtin fused attention.

## Instructions
1. pip install -e .
2. python benchmark.py